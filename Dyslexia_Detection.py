# -*- coding: utf-8 -*-
"""Dyslexia_Detection.ipynb

Automatically generated by Colab.

# ML based detection of Dyslexia (fMRI) #

## Clean Data ##
In order to get a clean data, we need to clean our data from outliers. \
The outliers of the global signal of each group (dyslexia,control) were identified in the notebook 'Scrubbing'. \
The 2 matrices 'control_outliers', 'dyslexia_outliers' contain the position of each outlier.

Next, we are going to clean all signals from outliers.
"""

import numpy as np
import pandas as pd
import sys
import os
from glob import glob
import matplotlib.pyplot as plt
import pickle
import seaborn as sns

"""## 1. Import all data corrected after GSR ##

## 3. Feature Extraction ##
At the next section, we will create the features for the model. \
Each feature represented by another coefficient in the correlation matrix. \
Steps:

- Chosing one/two ROIs to focus on.
- Extracting the upper triangle values to a flat vector.
- Represent all data as a panda DataFrame.

### Chosing one/two systems to focus on ###
In order to get a good classification, we need to have less features in our data.
If we will move forward to the next steps with the current number of ROIs as we have right now, we will have 20310 features, what will lead to **overfitting** for sure. \
Thus, we will chose to focus on certain systems to prevent overfitting.

<div class="alert alert-block alert-warning">
<b>Notice</b> We have 3 options for different systems: <br>
Group 1) cingulo-opercular and frontoparietal connectivity<br>
Group 2) dorsal and ventral attention network connectivity<br>
Group 3) default mode network connectivity
</div>
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
from sklearn import feature_selection
from sklearn.metrics import precision_score
# %cd /content/drive/MyDrive/features/

files = ["DAN_features_control_2.txt", "DAN_features_dyslexia_2.txt"]
list_files = []
for file in files:
  # if file.endswith('.txt'):

  # you can change '.csv' to any other type of file, but take into account the pd.read_csv
  # and change it to a pandas function that helps you read that type of file
  # e.g pd.read_excel in case it is a .xlsx file
  # or pd.read_json in case it is a .json file

  df = pd.read_csv(file, header=None)

    # you can add any piece of code inside this for loop in case
    # you want to create a cleaning pipeline

  list_files.append(df)

df = pd.concat(list_files, axis=0, ignore_index=True)
df

"""#### IV. Represent all data as a panda DataFrame"""

df

"""#### Droping columns which only have a single unique value

Such columns dont provide us any information. \
Notice: we chose to drop all columns with only 4 unique values.

"""

unique = df.apply(lambda x: len(x.unique()))
drop = unique.index[unique <= 4].tolist()
df = df.drop(drop,axis=1)
df

"""## Creating labels"""



control_list = [i for i in range(0,52)]
dyslexia_list = [i for i in range(52,117)]

# we will define 0 as control and 1 as dyslexia

#Y = df[[0]] #creating a dataframe with only 1 column

sub_list = df.index.values.tolist()
sub_list
sub_list = [i for i in sub_list] #creating a list of subjects numbers according to the data order
sub_list = np.asarray(sub_list,dtype=np.int64)
n = 117

x = 0
y = 0

y_df = []

for i in range (n):
    sub = sub_list[i]
    if sub in control_list:
        x +=1
        y_df.append((0))
    if sub in dyslexia_list:
        y+=1
        y_df.append((1))

X = df.copy()

Y = pd.DataFrame(y_df, index=df.index.copy())
Y = Y.rename(columns={0: "Label"})
Y



import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score
import random

# Load or define X_new and Y here
# X_new, Y

# Splitting the data into 'Train' & 'Test'
X_train, x_test, Y_train, y_test = train_test_split(X_new, Y.values.ravel(), test_size=0.20, shuffle=True, random_state=6, stratify=Y.values.ravel())

"""## Feature Selection ##
In a machine learning model itâ€™s rare that all the variables in the dataset are useful to build a model. Adding redundant variables reduces the generalization capability of the model and may also reduce the overall accuracy of a classifier. \
Furthermore adding more and more variables to a model increases the overall complexity of the model.

### Mutual Information

This method basically utilize the mutual information. It calculates mutual information value for each of independent variables with respect to dependent variable, and selects the ones which has most information gain. In other words, it basically measures the dependency of features with the target value. The higher score means more dependent variables.
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_selection import mutual_info_classif

# %matplotlib inline

importances = mutual_info_classif(X,Y.values.ravel())
feat_importances = pd.Series(importances, X.columns[0:len(X.columns)])

plt.figure(figsize = (10,8))
feat_importances.plot (kind='barh', color='teal')
plt.show()

feat_importances_high = feat_importances[feat_importances > 0.05]
feat_importances_i = feat_importances_high.index.to_numpy() #make the indices a list

#keep only features with high importance
X_new = X.loc[:,feat_importances_i]

X_new

"""### ANOVA F-value For Feature Selection

Compute the ANOVA F-value for the provided sample.
We will compute the ANOVA F-value between each feature and the target vector.
The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different.
"""

# Install necessary packages
!pip install pandas openpyxl scikit-learn

# Import required libraries
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif
from google.colab import files

# Example data (replace with your actual data)
# Assuming X_new and Y are already defined
# For demonstration, I'll create dummy data here
# X_new = pd.DataFrame(...) # your feature dataframe
# Y = pd.Series(...)        # your target series

# Select Features With Best ANOVA F-Values

# Create and fit selector
selector = SelectKBest(f_classif, k=115)
selector.fit(X_new, Y.values.ravel())

# Get columns to keep and create new dataframe with those only
cols = selector.get_support(indices=True)
X_new = X_new.iloc[:, cols]

# Export the resulting DataFrame to an Excel file
X_new.to_excel("selected_features.xlsx", index=False)

# Download the file
#files.download("selected_features.xlsx")

# Display the resulting DataFrame
X_new

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

# Select Features With Best ANOVA F-Values

# Create and fit selector
selector = SelectKBest(f_classif, k=115)
selector.fit(X_new, Y.values.ravel()
            )
# Get columns to keep and create new dataframe with those only
cols = selector.get_support(indices=True)
X_new = X_new.iloc[:,cols]

# Export the resulting DataFrame to an Excel file
X_new.to_excel("selected_features.xlsx", index=False)

X_new

"""### Confusion Matrix and Stats ###"""

# !pip uninstall scikit-learn -y
# !pip install scikit-learn

# import sklearn; print(sklearn.__version__)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
# from sklearn.metrics import plot_confusion_matrix

from sklearn.metrics import confusion_matrix
calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]
calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]
calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]
calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]
def stats(y_test, y_pred_test):
    TN = calc_TN(y_test, y_pred_test)
    FP = calc_FP(y_test, y_pred_test)
    FN = calc_FN(y_test, y_pred_test)
    TP = calc_TP(y_test, y_pred_test)
    Se = TP/(TP+FN)
    Sp = TN/(TN+FP)
    PPV = TP/(TP+FP)
    NPV = TN/(TN+FN)
    Acc = (TP+TN)/(TP+TN+FP+FN)
    F1 = (2*Se*PPV)/(Se+PPV)
    AUC = roc_auc_score(y_test, y_pred_test)
    return TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1, AUC

"""## SVM Algorithm #"""

## This code includes permutation of the labels in order to test the null hypothesis against the alternative hypothesis and get a significance level
# k-fold cross validation and algorithm implementation
n_splits = 3
skf = StratifiedKFold(n_splits=n_splits, random_state=2, shuffle=True)

svc = SVC(probability=True, random_state=10, max_iter=1000)
C = np.array([0.001, 0.01, 1, 10])

pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('svm', svc)])
svm = GridSearchCV(estimator=pipe,
                   param_grid={'svm__kernel': ['poly', 'rbf', 'linear'], 'svm__C': C},
                   scoring=['accuracy', 'f1', 'precision', 'roc_auc'],
                   cv=skf, refit='accuracy', verbose=3, return_train_score=True)

# Original fit
svm.fit(X_train, Y_train)
original_score = accuracy_score(y_test, svm.predict(x_test))
print("Original Accuracy:", original_score)

# Permutation test
n_permutations = 1000
permuted_scores = []
for i in range(n_permutations):
    # Shuffle labels
    Y_permuted = np.random.permutation(Y_train)

    # Fit model with permuted labels
    svm.fit(X_train, Y_permuted)
    permuted_score = accuracy_score(y_test, svm.predict(x_test))
    permuted_scores.append(permuted_score)
    print(f"Permuted Accuracy {i+1}: {permuted_score}")

# Calculate the average of the permuted scores
average_permuted_accuracy = np.mean(permuted_scores)
print("\n Original Accuracy:", original_score)
print(f"Average Permuted Accuracy: {average_permuted_accuracy:.4f}")

# Assess significance
permuted_scores = np.array(permuted_scores)
p_value = np.sum(permuted_scores >= original_score) / n_permutations
print("P-value:", p_value)

# k-fold cross validation and algorithm implementation
n_splits = 3
skf = StratifiedKFold(n_splits=n_splits, random_state=2, shuffle=True)

svc = SVC(probability=True, random_state=10, max_iter=1000)
C = np.array([0.001, 0.01, 1, 10])

pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('svm', svc)])
svm = GridSearchCV(estimator=pipe,
                       param_grid={'svm__kernel':['poly','rbf','linear'], 'svm__C':C},
                       scoring=['accuracy','f1','precision','roc_auc'],
                       cv=skf, refit='accuracy', verbose=3, return_train_score=True)
svm.fit(X_train, Y_train)

best_svm = svm.best_estimator_
print(svm.best_params_)

y_pred_test = best_svm.predict(x_test) #NOTICE NOT TO USE THE STANDARDIZED DATA.
y_pred_proba_test = best_svm.predict_proba(x_test)

TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1, AUC = stats(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
print('Sensitivity is {:.4f}. \nSpecificity is {:.4f}. \nPrecision is {:.2f}. \nPPV is {:.2f}. \nNPV is {:.4f}. \nAccuracy is {:.4f}. \nF1 is {:.4f}. \nAUC is {:.4f}.  '.format(Se,Sp,precision,PPV,NPV,Acc,F1,AUC))

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_test)

# Plot confusion matrix as heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Fit the model and obtain predictions
# (Assuming the relevant code is already provided before this point)
# ...

# Calculate the decision function scores
y_scores = best_svm.decision_function(x_test)

# Calculate the false positive rate (FPR) and true positive rate (TPR)
fpr, tpr, thresholds = roc_curve(y_test, y_scores)

# Calculate the AUC
auc = roc_auc_score(y_test, y_scores)

# Plot the ROC curve
plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = {:.4f})'.format(auc))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Compute false positive rate (FPR) and true positive rate (TPR) for different thresholds
y_svm_pred_proba_test = svm.predict_proba(x_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_svm_pred_proba_test)

# Calculate AUC
auc = roc_auc_score(y_test, y_pred_test)

# Plot ROC curve
plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = {:.4f})'.format(auc))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## RandomForest ##"""

## Random Forest Classifier with permutation test

import numpy as np
from sklearn.metrics import precision_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Fit model with original data
rfc = Pipeline(steps=[('scale', MinMaxScaler()), ('rfc', RandomForestClassifier(random_state=12, max_depth=5, criterion='gini'))])
rfc.fit(X_train, Y_train)
y_rf_pred_test = rfc.predict(x_test)

# Calculate original accuracy
original_accuracy = accuracy_score(y_test, y_rf_pred_test)
print(f'Original Accuracy: {original_accuracy:.4f}')

# Permutation test
n_permutations = 1000
permuted_accuracies = []

for _ in range(n_permutations):
    # Shuffle the labels
    print("permutation ", _)
    Y_permuted = np.random.permutation(Y_train)

    # Fit model with permuted labels
    rfc.fit(X_train, Y_permuted)
    permuted_pred = rfc.predict(x_test)
    permuted_accuracy = accuracy_score(y_test, permuted_pred)
    permuted_accuracies.append(permuted_accuracy)

# Calculate p-value
p_value = np.sum(np.array(permuted_accuracies) >= original_accuracy) / n_permutations

# Calculate and print average permuted accuracy
average_permuted_accuracy = np.mean(permuted_accuracies)
print(f'Original Accuracy: {original_accuracy:.4f}')
print(f'Average Permuted Accuracy: {average_permuted_accuracy:.4f}')
print(f'P-value from permutation test: {p_value:.4f}')

from sklearn.ensemble import RandomForestClassifier
rfc = Pipeline(steps=[('scale', MinMaxScaler()), ('rfc', RandomForestClassifier(random_state=12, max_depth=5,criterion='gini'))])
rfc.fit(X_train, Y_train)

y_rf_pred_test = rfc.predict(x_test)
y__rf_pred_proba_test = rfc.predict_proba(x_test)
# plot_confusion_matrix(rfc,x_test,y_test, cmap=plt.cm.Blues)

# TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1 = stats(y_test, y_rf_pred_test)
# print('Sensitivity is {:.4f}. \nSpecificity is {:.4f}. \nPPV is {:.2f}. \nNPV is {:.4f}. \nAccuracy is {:.4f}. \nF1 is {:.4f}. '.format(Se,Sp,PPV,NPV,Acc,F1))

TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1, AUC = stats(y_test, y_rf_pred_test)
precision = precision_score(y_test, y_rf_pred_test)
print('Sensitivity is {:.4f}. \nSpecificity is {:.4f}. \nPrecision is {:.2f}. \nPPV is {:.2f}. \nNPV is {:.4f}. \nAccuracy is {:.4f}. \nF1 is {:.4f}. '.format(Se,Sp,precision,PPV,NPV,Acc,F1))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_rf_pred_test = rfc.predict(x_test)
confusion_mat = confusion_matrix(y_test, y_rf_pred_test)

# Create the confusion matrix plot
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_rf_pred_proba_test = rfc.predict_proba(x_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_rf_pred_proba_test)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.plot(fpr, tpr, color='blue', label='ROC curve (AUC = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""##K-Nearest Neighbours (KNN)##"""

from sklearn.neighbors import KNeighborsClassifier
knn = Pipeline(steps=[('scale', MinMaxScaler()), ('knn', KNeighborsClassifier(n_neighbors=5))])
knn.fit(X_train, Y_train)

y_knn_pred_test = knn.predict(x_test)
y_knn_pred_proba_test = knn.predict_proba(x_test)
# plot_confusion_matrix(knn,x_test,y_test, cmap=plt.cm.Blues)

TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1, AUC = stats(y_test, y_knn_pred_test)
precision = precision_score(y_test, y_knn_pred_test)
print('Sensitivity is {:.4f}. \nSpecificity is {:.4f}. \nPrecision is {:.2f}. \nPPV is {:.2f}. \nNPV is {:.4f}. \nAccuracy is {:.4f}. \nF1 is {:.4f}. '.format(Se,Sp,precision,PPV,NPV,Acc,F1))

"""##Linear discriminant analysis (LDA)##"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = Pipeline(steps=[('scale', MinMaxScaler()), ('lda', LinearDiscriminantAnalysis())])
lda.fit(X_train, Y_train)

y_lda_pred_test = lda.predict(x_test)
y_lda_pred_proba_test = lda.predict_proba(x_test)
# plot_confusion_matrix(lda,x_test,y_test, cmap=plt.cm.Blues)

precision = precision_score(y_test, y_lda_pred_test)
TN, FP, FN, TP, Se, Sp, PPV, NPV, Acc, F1, AUC = stats(y_test, y_lda_pred_test)
print('Sensitivity is {:.4f}. \nSpecificity is {:.4f}. \nPrecision is {:.2f}. \nPPV is {:.2f}. \nNPV is {:.4f}. \nAccuracy is {:.4f}. \nF1 is {:.4f}. '.format(Se,Sp,precision,PPV,NPV,Acc,F1))

# from sklearn.metrics import plot_roc_curve, roc_auc_score

# classifiers = [best_svm, rfc, knn, lda]
# roc_score = []
# plt.figure()
# ax = plt.gca()
# for clf in classifiers:
#     plot_roc_curve(clf, x_test, y_test, ax=ax)
#     roc_score.append(np.round_(roc_auc_score(y_test, clf.predict_proba(x_test)[:,1]), decimals=4))
# ax.plot(np.linspace(0,1,x_test.shape[0]),np.linspace(0,1,x_test.shape[0]))
# plt.legend(('SVM, AUROC = '+str(roc_score[0]), 'Random Forest, AUROC = '+str(roc_score[1]),
#             'KNN, AUROC = '+str(roc_score[2]), 'LDA, AUROC = '+str(roc_score[3]), 'Flipping a Coin'))